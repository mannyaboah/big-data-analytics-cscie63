{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# HU Extension              Assignment 06       E63 Big Data Analytics \t\t                  \t\n",
                "## Handed out: 03/03/2023                                              Due by 11:59 AM EST, 03/11/2023\n",
                "## Student: Emmanuel Aboah"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Problem 1.\n",
                "Use NetCat utility on your Ubuntu VM or your operating system to generate a steady stream of short variable sentences (one to three English words long). Use and continuously increasing integer as the key for those messages. You can use any other utility or a small Python program to send that continuous stream of sentences to port 9999. Change the port if you have a good reason. Create a tool using Spark Structured Streaming API that will listen to that port and  pushes the count of different words used so far to the console. Choose the length of the period based on your convenience. If you are generating those sentences programmatically, the period should be short. If you are typing sentence by sentence, use longer period. Your sentence generation utility or/and Spark could reside either on Ubuntu VM or on your host operating system. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PySpark stream API consuming text messages on port 9999.\n",
                "\n",
                "# Imports\n",
                "import findspark\n",
                "from pyspark import SparkContext\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "findspark.init()\n",
                "\n",
                "sc = SparkContext.getOrCreate()\n",
                "\n",
                "sc.setLogLevel(\"ERROR\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "# count lines of words being published to port 9999.\n",
                "\n",
                "spark = SparkSession(sc)\n",
                "\n",
                "lines = (\n",
                "    spark.readStream\n",
                "    .format(\"socket\")\n",
                "    .option(\"host\", \"localhost\")\n",
                "    .option(\"port\", 9999)\n",
                "    .load()\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "from pyspark.sql.functions import *\n",
                "\n",
                "# Get words in lines and calculate counts\n",
                "words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
                "counts: DataFrame = words.groupBy(\"word\").count()\n",
                "\n",
                "# File system checkpoint\n",
                "checkpoint_dir = \"./checkpoint\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# streaming query\n",
                "streaming_query = (\n",
                "    counts.writeStream\n",
                "    .format(\"console\")\n",
                "    .outputMode(\"complete\")\n",
                "    .trigger(processingTime=\"5 second\")\n",
                "    .option(\"checkPointLocation\", checkpoint_dir)\n",
                "    .start()\n",
                ")\n",
                "\n",
                "streaming_query.awaitTermination()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```bash\n",
                "# Spark Stream sink to console output:\n",
                "\n",
                "-------------------------------------------\n",
                "Batch: 0\n",
                "-------------------------------------------\n",
                "+----+-----+\n",
                "|word|count|\n",
                "+----+-----+\n",
                "+----+-----+\n",
                "\n",
                "                                                                                \n",
                "-------------------------------------------\n",
                "Batch: 1\n",
                "-------------------------------------------\n",
                "+----+-----+\n",
                "|word|count|\n",
                "+----+-----+\n",
                "|  []|    1|\n",
                "+----+-----+\n",
                "\n",
                "                                                                                \n",
                "-------------------------------------------\n",
                "Batch: 2\n",
                "-------------------------------------------\n",
                "+----------+-----+\n",
                "|      word|count|\n",
                "+----------+-----+\n",
                "|[1:, more]|    1|\n",
                "|        []|    1|\n",
                "+----------+-----+\n",
                "\n",
                "                                                                                \n",
                "-------------------------------------------\n",
                "Batch: 3\n",
                "-------------------------------------------\n",
                "+----------+-----+\n",
                "|      word|count|\n",
                "+----------+-----+\n",
                "|[2:, more]|    1|\n",
                "|[1:, more]|    1|\n",
                "|        []|    1|\n",
                "+----------+-----+\n",
                "\n",
                "                                                                                \n",
                "-------------------------------------------\n",
                "Batch: 4\n",
                "-------------------------------------------\n",
                "+----------+-----+\n",
                "|      word|count|\n",
                "+----------+-----+\n",
                "|[2:, more]|    1|\n",
                "|[1:, more]|    1|\n",
                "|        []|    1|\n",
                "|[3:, test]|    1|\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Problem 2.\n",
                "Intercept messages produced by your “sentence generating utility” by Kafka and continuously write them to a Kafka topic. Write a Spark Structured Streaming application that would read that topic. Query the resulting Unbounded Table for the number of words read every minute or 5 minutes. Write results of such query to a database table in a MySQL database."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```bash\n",
                "# Create kafka topic for message write counts\n",
                "\n",
                "manny@LAPTOP-85L1BUVJ:~/dev/cscie-63/hw05/docker$ docker exec -it broker1 /bin/kafka-topics --bootstrap-server broker1:29092 --create --topic wordcounts --partitions 1\n",
                "\n",
                "Created topic wordcounts.\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Consume text stream messages and send them to a kafka topic table every 5 minutes."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```bash\n",
                "# Netcat utility to produce messages\n",
                "(base) manny@LAPTOP-85L1BUVJ:~$ nc -lk 9999\n",
                "data\n",
                "data\n",
                "data\n",
                "data\n",
                "data\n",
                "more\n",
                "more\n",
                "data\n",
                "more\n",
                "food\n",
                "food\n",
                "put\n",
                "put\n",
                "anything\n",
                "anything\n",
                "data\n",
                "data\n",
                "more\n",
                "anything\n",
                "food\n",
                "save\n",
                "put\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```python\n",
                "'''\n",
                "    🐍 App that reads text streams from tcp port 9999,\n",
                "    and publishes them to wordCounts kafka topic.\n",
                "'''\n",
                "from pyspark.sql import SparkSession, DataFrame\n",
                "from pyspark.sql.functions import split, col\n",
                "\n",
                "'''\n",
                "    Versions of packages\n",
                "'''\n",
                "scala_version = '2.12'\n",
                "spark_version = '3.3.1'\n",
                "\n",
                "'''\n",
                "    Import jars from maven central.\n",
                "    Follows Format -> groupId:artifactId:version\n",
                "'''\n",
                "packages = [\n",
                "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
                "    'org.apache.kafka:kafka-clients:3.3.1'\n",
                "]\n",
                "\n",
                "# Session Builder\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"StructuredWordCount\")\n",
                "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
                "    .getOrCreate()\n",
                ")\n",
                "\n",
                "lines = (\n",
                "    spark.readStream\n",
                "    .format(\"socket\")\n",
                "    .option(\"host\", \"localhost\")\n",
                "    .option(\"port\", 9999).load()\n",
                ")\n",
                "\n",
                "# Split the lines into words\n",
                "words = lines.select(split(col(\"value\"), \"\\\\s\").alias(\"word\"))\n",
                "\n",
                "# Generate running word count\n",
                "counts: DataFrame = words.groupBy(\"word\").count()\n",
                "checkpointDir = \"./checkpoint2\"\n",
                "\n",
                "# Sink counts streams to a kafka topic\n",
                "streamingQuery = (\n",
                "    counts.selectExpr(\"cast(word as string) as key\", \"cast(count as string) as value\")\n",
                "    .writeStream\n",
                "    .format(\"kafka\")\n",
                "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
                "    .option(\"topic\", \"wordCounts\")\n",
                "    .outputMode(\"update\")\n",
                "    .option(\"checkpointLocation\", checkpointDir)\n",
                "    .start()\n",
                ")\n",
                "\n",
                "streamingQuery.awaitTermination()\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```py\n",
                "'''\n",
                "    🐍 Structured Stream Kafka Consumer Mysql Sink App\n",
                "'''\n",
                "from pyspark.sql import SparkSession, DataFrame\n",
                "\n",
                "'''\n",
                "    Versions of packages\n",
                "'''\n",
                "scala_version = '2.12'\n",
                "spark_version = '3.3.1'\n",
                "kafka_version = '3.3.1'\n",
                "mysql_version = '8.0.31'\n",
                "\n",
                "'''\n",
                "    Import jars from maven central.\n",
                "    Follows Format -> groupId:artifactId:version\n",
                "'''\n",
                "packages = [\n",
                "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
                "    f'org.apache.kafka:kafka-clients:{kafka_version}',\n",
                "    f'mysql:mysql-connector-java:{mysql_version}'\n",
                "]\n",
                "\n",
                "# Session Builder\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"StructuredWordCount\")\n",
                "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
                "    .getOrCreate()\n",
                ")\n",
                "\n",
                "'''\n",
                "    User defined function to write streams to MySql in foreachBatch\n",
                "'''\n",
                "def my_sql_sink(df: DataFrame, batch_id: int):\n",
                "    url = \"jdbc:mysql://127.0.0.1:3306\"\n",
                "    (\n",
                "        df.write\n",
                "        .format(\"jdbc\")\n",
                "        .option(\"url\", url)\n",
                "        .option(\"dbtable\", \"db.wordCounts\")\n",
                "        .option(\"user\", \"root\")\n",
                "        .option(\"password\", \"password\")\n",
                "        .mode(\"append\")\n",
                "        .save()\n",
                "    )\n",
                "\n",
                "# Subscribe to wordCounts topic and convert key:value bytecode to string\n",
                "stream_df = (\n",
                "    spark.readStream\n",
                "    .format(\"kafka\")\n",
                "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
                "    .option(\"subscribe\", \"wordCounts\")\n",
                "    .load()\n",
                "    .selectExpr(\"CAST(key AS STRING) as word\", \"CAST(value AS STRING) as count\")\n",
                ")\n",
                "\n",
                "checkpointDir = \"./checkpoint_mysql\"\n",
                "\n",
                "stream_query = (\n",
                "    stream_df\n",
                "    .writeStream\n",
                "    .outputMode(\"update\")\n",
                "    .foreachBatch(my_sql_sink)\n",
                "    .option(\"checkpointLocation\", checkpointDir)\n",
                "    .trigger(processingTime=\"5 seconds\")\n",
                "    .start()\n",
                ")\n",
                "\n",
                "stream_query.awaitTermination()\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "MySQL Output\n",
                "\n",
                "```bash\n",
                "mysql> select * from wordCounts;\n",
                "+------------+-------+\n",
                "| word       | count |\n",
                "+------------+-------+\n",
                "| [anything] | 2     |\n",
                "| [data]     | 6     |\n",
                "| [more]     | 4     |\n",
                "| [anything] | 3     |\n",
                "| [food]     | 3     |\n",
                "| [put]      | 3     |\n",
                "| [save]     | 1     |\n",
                "+------------+-------+\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Problem 3.\n",
                "Modify your Spark Streaming application so it reads a fixed number of messages, for example 20. Write an “once trigger” that would start your Spark Structured Streaming application. Inside your Spark Structured Streaming application organize messages and the time stamps as short JSON objects with keys and values. Dump those messages in a memory table. Demonstrate that you can query that table and transfer its content into a regular Spark DataFrame."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```py\n",
                "'''\n",
                "🐍 App that streams collects messages from a kafka topic and sinks to in memory datasource.\n",
                "'''\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import to_json, struct\n",
                "\n",
                "'''\n",
                "    Versions of packages\n",
                "'''\n",
                "scala_version = \"2.12\"\n",
                "spark_version = \"3.3.1\"\n",
                "kafka_version = \"3.3.1\"\n",
                "mysql_version = \"8.0.31\"\n",
                "\n",
                "'''\n",
                "    Import jars from maven central.\n",
                "    Follows Format -> groupId:artifactId:version\n",
                "'''\n",
                "packages = [\n",
                "    f\"org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}\",\n",
                "    f\"org.apache.kafka:kafka-clients:{kafka_version}\",\n",
                "    f\"mysql:mysql-connector-java:{mysql_version}\"\n",
                "]\n",
                "\n",
                "# Session Builder\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"StructuredWordCount\")\n",
                "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
                "    .getOrCreate()\n",
                ")\n",
                "\n",
                "# Subscribe to wordcounts with offset set to earliest\n",
                "stream_df = (\n",
                "    spark.readStream\n",
                "    .format(\"kafka\")\n",
                "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
                "    .option(\"subscribe\", \"wordCounts\")\n",
                "    .option(\"startingOffsets\", \"earliest\")\n",
                "    .load()\n",
                ")\n",
                "\n",
                "# Convert topic values and select required columns and filter for latest messages.\n",
                "stream_df = (\n",
                "    stream_df\n",
                "    .withColumn(\"word\", stream_df[\"key\"].cast(\"string\").alias(\"word\"))\n",
                "    .drop(\"key\")\n",
                "    .withColumn(\"count\", stream_df[\"value\"].cast(\"string\").alias(\"count\"))\n",
                "    .drop(\"value\")\n",
                "    .select(\"word\", \"count\", \"timestamp\")\n",
                "    .where(\"timestamp >= '2023-03-12 19:00:00'\")\n",
                "    .limit(20)\n",
                ")\n",
                "\n",
                "\n",
                "# Convert to json\n",
                "stream_df_json = (\n",
                "    stream_df\n",
                "    .withColumn(\"words_json\", to_json(struct(\"word\", \"count\", \"timestamp\")))\n",
                "    .select(\"words_json\")\n",
                ")\n",
                "\n",
                "\n",
                "# In memory sink\n",
                "stream_query = (\n",
                "    stream_df_json\n",
                "    .writeStream\n",
                "    .format(\"memory\")\n",
                "    .queryName(\"word_counts\")\n",
                "    .outputMode(\"append\")\n",
                "    .trigger(once=True)\n",
                "    .start()\n",
                ")\n",
                "\n",
                "stream_query.awaitTermination()\n",
                "\n",
                "spark.sql(\"select * from word_counts\").show()\n",
                "```\n",
                "\n",
                "Output from DF\n",
                "\n",
                "```bash\n",
                "+--------------------+                                                          \n",
                "|          words_json|\n",
                "+--------------------+\n",
                "|{\"word\":\"[count]\"...|\n",
                "|{\"word\":\"[test]\",...|\n",
                "|{\"word\":\"[test]\",...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "|{\"word\":\"[test]\",...|\n",
                "|{\"word\":\"[county]...|\n",
                "|{\"word\":\"[country...|\n",
                "|{\"word\":\"[count]\"...|\n",
                "|{\"word\":\"[county]...|\n",
                "|{\"word\":\"[country...|\n",
                "|{\"word\":\"[count]\"...|\n",
                "|{\"word\":\"[county]...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "|{\"word\":\"[test]\",...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "|{\"word\":\"[county]...|\n",
                "|{\"word\":\"[test]\",...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "|{\"word\":\"[money]\"...|\n",
                "+--------------------+\n",
                "```"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Problem 4.\n",
                "Transform your trigger into a processing time trigger that fires every 2 minutes and upends messages it reads to a MySQL table. Prove that the number of messages in the table continuously grow."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```py\n",
                "'''\n",
                "🐍 App that reads pre-aggregated word counts from a kafka \n",
                "Topic and sinks to MySql as json field.\n",
                "'''\n",
                "from pyspark.sql import SparkSession, DataFrame\n",
                "from pyspark.sql.functions import to_json, struct\n",
                "\n",
                "'''\n",
                "    Versions of packages\n",
                "'''\n",
                "scala_version = '2.12'\n",
                "spark_version = '3.3.1'\n",
                "kafka_version = '3.3.1'\n",
                "mysql_version = '8.0.31'\n",
                "\n",
                "'''\n",
                "    Import jars from maven central.\n",
                "    Follows Format -> groupId:artifactId:version\n",
                "'''\n",
                "packages = [\n",
                "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
                "    f'org.apache.kafka:kafka-clients:{kafka_version}',\n",
                "    f'mysql:mysql-connector-java:{mysql_version}'\n",
                "]\n",
                "\n",
                "# Build Session\n",
                "spark = (\n",
                "    SparkSession.builder\n",
                "    .appName(\"StructuredWordCount\")\n",
                "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
                "    .getOrCreate()\n",
                ")\n",
                "\n",
                "\n",
                "# User defined function to write streams to MySql in foreachBatch\n",
                "def my_sql_sink(df: DataFrame, batch_id: int):\n",
                "    url = \"jdbc:mysql://127.0.0.1:3306\"\n",
                "    (\n",
                "        df.write\n",
                "        .format(\"jdbc\")\n",
                "        .option(\"url\", url)\n",
                "        .option(\"dbtable\", \"db.wordCountsJson\")\n",
                "        .option(\"user\", \"root\")\n",
                "        .option(\"password\", \"password\")\n",
                "        .mode(\"append\")\n",
                "        .save()\n",
                "    )\n",
                "\n",
                "# Subscribe to wordCounts topic with offset at latest.\n",
                "stream_df = (\n",
                "    spark.readStream\n",
                "    .format(\"kafka\")\n",
                "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
                "    .option(\"subscribe\", \"wordCounts\")\n",
                "    .option(\"startingOffsets\", \"latest\")\n",
                "    .load()\n",
                ")\n",
                "\n",
                "# Convert topic values and select required columns.\n",
                "stream_df = (\n",
                "    stream_df\n",
                "    .withColumn(\"word\", stream_df[\"key\"].cast(\"string\").alias(\"word\"))\n",
                "    .drop(\"key\")\n",
                "    .withColumn(\"count\", stream_df[\"value\"].cast(\"string\").alias(\"count\"))\n",
                "    .drop(\"value\")\n",
                ")\n",
                "\n",
                "# Convert to json\n",
                "stream_df_json = (\n",
                "    stream_df\n",
                "    .withColumn(\"words_json\", to_json(struct(\"word\", \"count\", \"timestamp\")))\n",
                "    .select(\"words_json\")\n",
                ")\n",
                "\n",
                "# Checkpoint directory for running job.\n",
                "checkpointDir = \"./checkpoint_mysql\"\n",
                "\n",
                "# MySql Sink updates date on 2 minute trigger\n",
                "stream_query = (\n",
                "    stream_df_json\n",
                "    .writeStream\n",
                "    .outputMode(\"update\")\n",
                "    .foreachBatch(my_sql_sink)\n",
                "    .option(\"checkpointLocation\", checkpointDir)\n",
                "    .trigger(processingTime=\"2 minutes\")\n",
                "    .start()\n",
                ")\n",
                "\n",
                "stream_query.awaitTermination()\n",
                "```\n",
                "\n",
                "Output in MySql wordCountsJson Table.\n",
                "\n",
                "```bash\n",
                "mysql> select * from wordCountsJson;\n",
                "+---------------------------------------------------------------------------------+\n",
                "| words_json                                                                      |\n",
                "+---------------------------------------------------------------------------------+\n",
                "| {\"word\": \"[bate]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:42:25.915-04:00\"}  |\n",
                "| {\"word\": \"[money]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:42:31.022-04:00\"} |\n",
                "| {\"word\": \"[put]\", \"count\": \"1\", \"timestamp\": \"2023-03-13T16:42:35.570-04:00\"}   |\n",
                "| {\"word\": \"[test]\", \"count\": \"1\", \"timestamp\": \"2023-03-13T16:42:38.185-04:00\"}  |\n",
                "| {\"word\": \"[put]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:42:40.466-04:00\"}   |\n",
                "| {\"word\": \"[money]\", \"count\": \"3\", \"timestamp\": \"2023-03-13T16:42:40.838-04:00\"} |\n",
                "| {\"word\": \"[test]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:42:43.369-04:00\"}  |\n",
                "| {\"word\": \"[put]\", \"count\": \"3\", \"timestamp\": \"2023-03-13T16:42:45.597-04:00\"}   |\n",
                "| {\"word\": \"[bate]\", \"count\": \"3\", \"timestamp\": \"2023-03-13T16:42:50.828-04:00\"}  |\n",
                "| {\"word\": \"[test]\", \"count\": \"3\", \"timestamp\": \"2023-03-13T16:42:58.595-04:00\"}  |\n",
                "| {\"word\": \"[many]\", \"count\": \"1\", \"timestamp\": \"2023-03-13T16:43:28.454-04:00\"}  |\n",
                "| {\"word\": \"[more]\", \"count\": \"1\", \"timestamp\": \"2023-03-13T16:43:40.598-04:00\"}  |\n",
                "| {\"word\": \"[many]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:43:42.867-04:00\"}  |\n",
                "| {\"word\": \"[more]\", \"count\": \"2\", \"timestamp\": \"2023-03-13T16:43:45.567-04:00\"}  |\n",
                "+---------------------------------------------------------------------------------+\n",
                "14 rows in set (0.00 sec)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.9"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
