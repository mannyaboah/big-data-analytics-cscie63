{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HU Extension       Assignment 12             E63 Big Data Analytics \t\t                  \t\n",
    "## Handed out: 04/21/2023                           Due by 11:59 PM EST on Saturday, 04/29/2023\n",
    "### Emmanuel Aboah"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n",
    "\n",
    "Install delta-spark Python package. Confirm that your installation is valid by creating a table in the local file system. Report on the location of the files (directories) containing your data. How many parquet files you created with the first save operation. Demonstrate that you can read saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import findspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\"\"\"\n",
    "    Import jars from maven central.\n",
    "    Follows Format -> groupId:artifactId:version\n",
    "\"\"\"\n",
    "packages = [\"io.delta:delta-core_2.12:2.3.0\"]\n",
    "\n",
    "# Session Builder\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"delta_lakes_app\")\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages))\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create in local file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "\n",
    "test_data = spark.range(0, 15)\n",
    "\n",
    "test_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"output_data/test-table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save to filesystem\n",
    "(\n",
    "    test_data.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(OUTPUT_PATH)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report on the location of the files (directories) containing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 76\n",
      "drwxr-xr-x 3 manny manny 4096 Apr 28 18:51 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
      "drwxr-xr-x 3 manny manny 4096 Apr 28 18:51 \u001b[01;34m..\u001b[0m/\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00000-c8b93c83-d679-4371-9bd0-74fc258443cf-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00001-b21482c7-0789-41e3-b162-86f6f4ac63b4-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00002-464b4fb0-65bb-4300-b6a5-757bbdc7ea01-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00003-8c9de733-d20d-45b0-ad4f-a75d9d75627a-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00004-7ba3280b-9cfa-46f2-837c-cbaa7c082b3f-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00005-d367405b-684e-489e-a26c-0451434cb4ab-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00006-66c8138a-abfa-4ff8-a6e2-655e538071e7-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 manny manny   12 Apr 28 18:51 .part-00007-8bf98dc1-b13f-42a5-86c6-fedce667727f-c000.snappy.parquet.crc\n",
      "drwxr-xr-x 2 manny manny 4096 Apr 28 18:51 \u001b[01;34m_delta_log\u001b[0m/\n",
      "-rw-r--r-- 1 manny manny  478 Apr 28 18:51 part-00000-c8b93c83-d679-4371-9bd0-74fc258443cf-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00001-b21482c7-0789-41e3-b162-86f6f4ac63b4-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00002-464b4fb0-65bb-4300-b6a5-757bbdc7ea01-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00003-8c9de733-d20d-45b0-ad4f-a75d9d75627a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00004-7ba3280b-9cfa-46f2-837c-cbaa7c082b3f-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00005-d367405b-684e-489e-a26c-0451434cb4ab-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00006-66c8138a-abfa-4ff8-a6e2-655e538071e7-c000.snappy.parquet\n",
      "-rw-r--r-- 1 manny manny  486 Apr 28 18:51 part-00007-8bf98dc1-b13f-42a5-86c6-fedce667727f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "ls output_data/test-table -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "ls -1A output_data/test-table | wc -l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate that you can read saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "|  5|\n",
      "|  6|\n",
      "|  3|\n",
      "|  4|\n",
      "|  9|\n",
      "| 10|\n",
      "|  7|\n",
      "|  8|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_df = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "test_data_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. \n",
    "\n",
    "Go through 4 successive operations: one update, one insert, one merge and one delete of the data in your delta table created in Problem 1. Use time travel to demonstrate that you see the state of the table before each operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr, col\n",
    "\n",
    "# Create a Delta Table\n",
    "delta_table = DeltaTable.forPath(spark, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update table by multiplying each value by 2\n",
    "delta_table.update(set={\"id\": expr(\"id * 2\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  6|\n",
      "|  8|\n",
      "| 26|\n",
      "| 28|\n",
      "| 18|\n",
      "| 20|\n",
      "| 10|\n",
      "| 12|\n",
      "| 22|\n",
      "| 24|\n",
      "| 14|\n",
      "| 16|\n",
      "|  2|\n",
      "|  4|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upd_df = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "upd_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 14|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "|  6|\n",
      "|  8|\n",
      "| 26|\n",
      "| 28|\n",
      "| 10|\n",
      "| 12|\n",
      "| 22|\n",
      "| 24|\n",
      "|  2|\n",
      "|  4|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Upsert/ merge the numbers 16 - 20\n",
    "new_data = spark.range(16, 21)\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"prev_data\")\n",
    "    .merge(\n",
    "        new_data.alias(\"new_data\"),\n",
    "        \"prev_data.id = new_data.id\")\n",
    "    .whenMatchedUpdate(set= {\"id\": col(\"new_data.id\")})\n",
    "    .whenNotMatchedInsert(values= {\"id\": col(\"new_data.id\")})\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 14|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "|  6|\n",
      "|  8|\n",
      "| 10|\n",
      "| 12|\n",
      "|  2|\n",
      "|  4|\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Delete records > 19\n",
    "delta_table.delete(condition=expr(\"id > 19\"))\n",
    "\n",
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use time travel to show different version states\n",
    "\n",
    "# version 1\n",
    "df1 = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\", 0)\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "df1.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "| 10|\n",
      "| 12|\n",
      "| 14|\n",
      "| 16|\n",
      "| 18|\n",
      "| 20|\n",
      "| 22|\n",
      "| 24|\n",
      "| 26|\n",
      "| 28|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "df2 = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\", 1)\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "df2.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "| 10|\n",
      "| 12|\n",
      "| 14|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "| 22|\n",
      "| 24|\n",
      "| 26|\n",
      "| 28|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# version 3\n",
    "df1 = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\", 2)\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "df1.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "| 10|\n",
      "| 12|\n",
      "| 14|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# version 4\n",
    "df1 = (\n",
    "    spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"versionAsOf\", 3)\n",
    "    .load(OUTPUT_PATH)\n",
    ")\n",
    "\n",
    "df1.orderBy(\"id\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. \n",
    "\n",
    "Create a standalone Python script that will perform a conditional uperst on your table in Problem 2. Let the script query and print the content of the table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "\"\"\"🐍 __python__ script to perform upsert as per problem 3\n",
    "\"\"\"\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_PATH = \"output_data/test-table\"\n",
    "\n",
    "# Create a builder with the Delta extensions\n",
    "builder = (\n",
    "    SparkSession.builder.appName(\"delta_lake_app\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a Spark instance with the builder\n",
    "# As a result, we now can read and write Delta files\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Create a Delta Table\n",
    "delta_table = DeltaTable.forPath(spark, OUTPUT_PATH)\n",
    "\n",
    "# Show table prior to upsert\n",
    "print(\"Test Delta Table prior to upsert\")\n",
    "delta_table.toDF().show()\n",
    "\n",
    "# Upsert/ merge the numbers 16 - 20\n",
    "new_data = spark.range(20, 41)\n",
    "\n",
    "(\n",
    "    delta_table.alias(\"prev_data\")\n",
    "    .merge(new_data.alias(\"new_data\"), \"prev_data.id = new_data.id\")\n",
    "    .whenMatchedUpdate(set={\"id\": col(\"new_data.id\")})\n",
    "    .whenNotMatchedInsert(values={\"id\": col(\"new_data.id\")})\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "# Show updated data\n",
    "print(\"Updated Delta Table\")\n",
    "\n",
    "delta_table.toDF().show()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the python script\n",
    "\n",
    "```bash\n",
    "python delta_lake_upserts.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "\n",
    "```bash\n",
    "(BDA2023) manny@LAPTOP-85L1BUVJ:~/dev/cscie-63/hw12_delta_lakes$ python delta_lake_upserts.py\n",
    "23/04/28 18:52:58 WARN Utils: Your hostname, LAPTOP-85L1BUVJ resolves to a loopback address: 127.0.1.1; using 172.19.201.226 instead (on interface eth0)\n",
    "23/04/28 18:52:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    ":: loading settings :: url = jar:file:/home/manny/dev/spark-3.3.1-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
    "Ivy Default Cache set to: /home/manny/.ivy2/cache\n",
    "The jars for the packages stored in: /home/manny/.ivy2/jars\n",
    "io.delta#delta-core_2.12 added as a dependency\n",
    ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9cb11ddc-5aa3-4205-a825-ed38e2e85a69;1.0\n",
    "        confs: [default]\n",
    "        found io.delta#delta-core_2.12;2.3.0 in central\n",
    "        found io.delta#delta-storage;2.3.0 in central\n",
    "        found org.antlr#antlr4-runtime;4.8 in central\n",
    ":: resolution report :: resolve 468ms :: artifacts dl 22ms\n",
    "        :: modules in use:\n",
    "        io.delta#delta-core_2.12;2.3.0 from central in [default]\n",
    "        io.delta#delta-storage;2.3.0 from central in [default]\n",
    "        org.antlr#antlr4-runtime;4.8 from central in [default]\n",
    "        ---------------------------------------------------------------------\n",
    "        |                  |            modules            ||   artifacts   |\n",
    "        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
    "        ---------------------------------------------------------------------\n",
    "        |      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
    "        ---------------------------------------------------------------------\n",
    ":: retrieving :: org.apache.spark#spark-submit-parent-9cb11ddc-5aa3-4205-a825-ed38e2e85a69\n",
    "        confs: [default]\n",
    "        0 artifacts copied, 3 already retrieved (0kB/22ms)\n",
    "23/04/28 18:53:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "23/04/28 18:53:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Test Delta Table prior to upsert                                                \n",
    "23/04/28 18:53:21 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
    "+---+                                                                           \n",
    "| id|\n",
    "+---+\n",
    "| 14|\n",
    "| 16|\n",
    "| 17|\n",
    "| 18|\n",
    "| 19|\n",
    "|  6|\n",
    "|  8|\n",
    "| 10|\n",
    "| 12|\n",
    "|  2|\n",
    "|  4|\n",
    "|  0|\n",
    "+---+\n",
    "\n",
    "23/04/28 18:53:39 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
    "23/04/28 18:53:40 WARN HintErrorLogger: Hint (strategy=broadcast) is not supported in the query: build left for full outer join.\n",
    "Updated Delta Table                                                             \n",
    "+---+                                                                           \n",
    "| id|\n",
    "+---+\n",
    "| 21|\n",
    "| 20|\n",
    "| 22|\n",
    "| 23|\n",
    "| 24|\n",
    "| 26|\n",
    "| 25|\n",
    "| 29|\n",
    "| 27|\n",
    "| 28|\n",
    "| 32|\n",
    "| 31|\n",
    "| 30|\n",
    "| 34|\n",
    "| 33|\n",
    "| 37|\n",
    "| 35|\n",
    "| 36|\n",
    "| 39|\n",
    "| 38|\n",
    "+---+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. \n",
    "\n",
    "Search through delta.io documentation to find the way to describe your table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "|format|                  id|name|description|            location|           createdAt|        lastModified|partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|       tableFeatures|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "| delta|f6fbe204-b060-4a5...|null|       null|file:/home/manny/...|2023-04-28 18:51:...|2023-04-28 18:53:...|              []|       6|       3011|        {}|               1|               2|[appendOnly, inva...|\n",
      "+------+--------------------+----+-----------+--------------------+--------------------+--------------------+----------------+--------+-----------+----------+----------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe table\n",
    "\n",
    "delta_table.detail().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. \n",
    "\n",
    "Search through delta.io documentation to find the way to print the history of your table. Present the timestamp when the operation was performed and the type of the operation performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|           timestamp|operation|\n",
      "+--------------------+---------+\n",
      "|2023-04-28 18:53:...|    MERGE|\n",
      "|2023-04-28 18:52:...|   DELETE|\n",
      "|2023-04-28 18:52:...|    MERGE|\n",
      "|2023-04-28 18:51:...|   UPDATE|\n",
      "|2023-04-28 18:51:...|    WRITE|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history_df = delta_table.history()\n",
    "\n",
    "history_df.select(\"timestamp\", \"operation\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BDA2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
